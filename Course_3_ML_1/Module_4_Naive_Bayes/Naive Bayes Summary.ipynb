{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d454df5",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "\n",
    "## Conditional Probability\n",
    "The probability of one event given that occurance of another event is called conditional probability P(A|B). \n",
    "\n",
    "## Joint Probability\n",
    "The Probability of two (or more) events is called Joint probability P(A ∩ B).\n",
    "\n",
    "## Marginal Probability\n",
    "The probability of an event in presence of all (or subset of) the outcomes of the other random variable is called marginal probability.\n",
    "\n",
    "## Prior\n",
    "Its the probability of Event A in isolation.\n",
    "\n",
    "## Posterior\n",
    "Its the probability of Event A when Event B occurs.\n",
    "\n",
    "## Contingency table\n",
    "Its the table containing frequncy of all the outcomes of two events A and B. \n",
    "e.g. Event A - Sachin century\n",
    "Event B - India match result\n",
    "\n",
    "For Event A, the possible outcomes are Sachin scores century or does not score century. For Event B, the two possible outcomes are India wins or India losses. So the contegency matrix will be as follows\n",
    "\n",
    "| | India wins  | India losses|\n",
    "|-|-------------|-------------|\n",
    "|Sachin scores century| 10 | 2 |\n",
    "|Sachin does not score century | 50 | 38|\n",
    "Total | 60 | 40\n",
    "\n",
    "\n",
    "\n",
    "# Bayes Theorem\n",
    "$P(Ci|x) = \\frac {P(x|Ci)P(Ci)}{P(x)}$ , where Ci denotes the classes, and X denotes the features of the data point.\n",
    "\n",
    "$P(Ci)$ is the prior probability. Its the probability of an event occuring before collection of new data points. It highly influences the class of the new data point.\n",
    "\n",
    "$P(x|Ci)$ represents likelihood function. Its likelihood of a new data point occuring in the category. The conditional independence asusmption is leveraged while computing the likelihod function.\n",
    "\n",
    "$P(x)$ the effect of denominator is not incorporated in computing probabilitiies as its same for both the classes.\n",
    "\n",
    "$P(Ci|x)$ is the posterior probability. Its compared for final classifiction. The new data point is assigned to a class with greater posterior probability.\n",
    "\n",
    "\n",
    "## Conditional Independence\n",
    "Two event A and B are said to be conditionally independent if P(A) is same for all values of B and vice versa.\n",
    "This can be used in computing the P(A and B|C) as P(A|C) times P(B|C).\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "## Assumptions\n",
    "1. Variables i.e. their probabilities are independent given class.\n",
    "\n",
    "Due to this assumption, its called Naive Bayes.\n",
    "\n",
    "\n",
    "# Naive Bayes in Text classification\n",
    "\n",
    "## Bag of words \n",
    "- Build vocabulary of unique words in given text data set.\n",
    "- Build feature vector capturing the frequency of words in text.\n",
    "- This representation is called bag of words.\n",
    "- Represented by $|v|$\n",
    "\n",
    "## Probability calculation\n",
    "\n",
    "$ P(word | class) = \\frac {Frequency of word in class} {Number of words in the class} $\n",
    "\n",
    "e.g. if word good occurs 3 times in a class and total number of words in the class are 16, the P(good|class) = 3/16.\n",
    "\n",
    "## Laplace Smoothing\n",
    "The problem with Bayes is that if a particular word which is part of a class is not present in a text, then the probability for that text being classified as a particular class will be zero. In order to solve this problem we add one to the frequencies of all the words in bag of words representation. This is callled as Laplace smoothing. Thus we end up adding the size of vocabulary in the total size of bag of words. \\\n",
    "Instead of adding 1, we can also add alpha (α) to each word frequency. In that case we end up adding $α * |v|$ to our vocabulary. \n",
    "\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "- The feature vector captures if the word is present in given text or not\n",
    "- Represented by 1 if the word is present and 0 if its not\n",
    "\n",
    "## Computation of probability\n",
    "For Bernouli Naive Bayes, the probability calculation differes from Multinomial Naive Bayes. \n",
    "\n",
    "$ P(word | class) \\frac {Frequency of word in class} {Number of documents in the class}$\n",
    "\n",
    "e.g. if word good occurs 3 times in a class and total number of documents in the class are 8, the P(good|class) = 3/8\n",
    "\n",
    "## Laplace Smoothing for Bernouli Naive Bayes\n",
    "The formula for calculating probability of a word is \n",
    "\n",
    "$ P(w_t, C) = \\frac {n_c(w_t) + 1}{N_c+2)$ \n",
    "\n",
    "Where, $n_c$ is the number of documents the word $w_t$ occurs\n",
    "$ N_c $ is the number of documents in class C\n",
    "\n",
    "\n",
    "## Algorithm\n",
    "1. Construct the feature vector d like {0,1,0,0,1,1,1,0} for the document\n",
    "2. We only consider words which are in our vocabulary\n",
    "3. $d_i$ is boolean indicating if a word is present in the feature vector for the document \n",
    "3. If the word is present in feature vector, its probability is $ d_i * P(w_t, C)$ \n",
    "4. If word is not present in feature vector, its probability is $ (1-di) * (1- P(w_t, C))$\n",
    "\n",
    "the generalized formula is:\\\n",
    "$ P(w_1,w_2,..w_n,C) = P(d|C) = \\prod_{i=1}^n [d_iP(w_t,C)+(1-d_i)(1-P(w_t,C))]$\n",
    "\n",
    "\n",
    "## Difference between Bernouli Naive Bayes and Multinomial Naive Bayes\n",
    "- Multinomial Naive Bayes takes frequency of words into consideration while Bernouli Naive Bayes does not consider frequency rather relies just on presence of the word.\n",
    "- Bernouli Naive Bayes takes into consideration the non-occurring terms while Multinomial Naive Bayes does not consder terms which are not in the document.\n",
    "\n",
    "\n",
    "# Bayesian Estimate\n",
    "\n",
    "\n",
    "## Difference with Maximum Likelihood estimate\n",
    "\n",
    "# Notes\n",
    "1. The independence assumption of Naive Bayes is different than of Linear regression. The difference is for Naive Bayes the assumption is that probability of two events are independent and for Linear regression the independence means that the variables do not have correlation. Its statistical independence and other is probabilistic independene. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
